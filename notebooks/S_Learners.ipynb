{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = pd.read_csv('../data/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10391 entries, 0 to 10390\n",
      "Data columns (total 13 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   schoolid  10391 non-null  int64  \n",
      " 1   Z         10391 non-null  int64  \n",
      " 2   Y         10391 non-null  float64\n",
      " 3   S3        10391 non-null  int64  \n",
      " 4   C1        10391 non-null  int64  \n",
      " 5   C2        10391 non-null  int64  \n",
      " 6   C3        10391 non-null  int64  \n",
      " 7   XC        10391 non-null  int64  \n",
      " 8   X1        10391 non-null  float64\n",
      " 9   X2        10391 non-null  float64\n",
      " 10  X3        10391 non-null  float64\n",
      " 11  X4        10391 non-null  float64\n",
      " 12  X5        10391 non-null  float64\n",
      "dtypes: float64(6), int64(7)\n",
      "memory usage: 1.0 MB\n",
      "None\n",
      "Dataset Description:\n",
      "           schoolid             Z             Y            S3            C1  \\\n",
      "count  10391.000000  10391.000000  10391.000000  10391.000000  10391.000000   \n",
      "mean      39.888846      0.325666     -0.096742      5.268117      5.223078   \n",
      "std       24.008975      0.468646      0.643009      1.120765      3.982963   \n",
      "min        1.000000      0.000000     -2.097420      1.000000      1.000000   \n",
      "25%       19.000000      0.000000     -0.548980      5.000000      3.000000   \n",
      "50%       41.000000      0.000000     -0.118923      5.000000      4.000000   \n",
      "75%       62.000000      1.000000      0.335663      6.000000      5.000000   \n",
      "max       76.000000      1.000000      2.194709      7.000000     15.000000   \n",
      "\n",
      "                 C2            C3            XC            X1            X2  \\\n",
      "count  10391.000000  10391.000000  10391.000000  10391.000000  10391.000000   \n",
      "mean       1.489943      0.630931      2.447791     -0.040457      0.054841   \n",
      "std        0.499923      0.482576      1.378420      0.969743      0.935560   \n",
      "min        1.000000      0.000000      0.000000     -3.088790     -3.347819   \n",
      "25%        1.000000      0.000000      1.000000     -0.617888     -0.544506   \n",
      "50%        1.000000      1.000000      2.000000     -0.009954     -0.022514   \n",
      "75%        2.000000      1.000000      4.000000      0.420441      0.726836   \n",
      "max        2.000000      1.000000      4.000000      2.834589      2.171815   \n",
      "\n",
      "                 X3            X4            X5  \n",
      "count  10391.000000  10391.000000  10391.000000  \n",
      "mean      -0.089349     -0.045911     -0.026168  \n",
      "std        0.962804      0.967262      1.010387  \n",
      "min       -1.575463     -1.924778     -1.805073  \n",
      "25%       -0.963095     -0.813799     -0.857026  \n",
      "50%       -0.057036     -0.159602     -0.211553  \n",
      "75%        0.515392      0.596474      0.847844  \n",
      "max        2.358274      2.821660      1.892348  \n"
     ]
    }
   ],
   "source": [
    "# Display basic info of the dataset to check for missing values and data types\n",
    "print(\"Dataset Information:\")\n",
    "print(dataset.info())\n",
    "print(\"Dataset Description:\")\n",
    "print(dataset.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(dataset):\n",
    "\n",
    "    # Display basic info of the dataset to check for missing values and data types\n",
    "    print(\"Dataset Information:\")\n",
    "    print(dataset.info())\n",
    "    print(\"Dataset Description:\")\n",
    "    print(dataset.describe())\n",
    "\n",
    "    # Step 1: Feature Engineering (optional, you can add more interactions based on domain knowledge)\n",
    "    # Example: Creating an interaction feature between mindset and school achievement level\n",
    "    dataset['Mindset_School_Achievement_Interaction'] = dataset['X1'] * dataset['X2']\n",
    "\n",
    "    # Step 2: Define covariates (features), treatment, and outcome\n",
    "    outcome_col = 'Y'  # Student Achievement Score (Outcome)\n",
    "    treatment_col = 'Z'  # Growth Mindset Intervention (Treatment)\n",
    "    covariate_cols = ['S3', 'C1', 'C2', 'C3', 'XC', 'X1', 'X2', 'X3', 'X4', 'X5', 'Mindset_School_Achievement_Interaction']\n",
    "\n",
    "    # Step 3: Categorical and Numerical Feature Identification\n",
    "    # Categorical columns that need one-hot encoding\n",
    "    categorical_cols = ['C1', 'C2', 'C3', 'XC']\n",
    "\n",
    "    # Numerical columns that need to be standardized\n",
    "    numerical_cols = ['X1', 'X2', 'X3', 'X4', 'X5', 'Mindset_School_Achievement_Interaction']\n",
    "\n",
    "    # Step 4: Preprocessing Pipeline\n",
    "    # A pipeline that will scale numerical features and one-hot encode categorical ones\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_cols),  # Scaling numerical columns\n",
    "            ('cat', OneHotEncoder(drop='first'), categorical_cols)  # One-hot encoding categorical columns\n",
    "        ])\n",
    "\n",
    "    # Step 5: Train-Test Split\n",
    "    X = dataset[covariate_cols]  # Features (covariates)\n",
    "    y = dataset[outcome_col]  # Outcome (Student Achievement Score)\n",
    "\n",
    "    # Split data into training and test sets (80% train, 20% test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Step 6: Apply the Preprocessing Pipeline\n",
    "    # Fit the preprocessor on the training data and transform both train and test sets\n",
    "    X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "    X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "    # Retrieve the feature names after one-hot encoding and scaling\n",
    "    # This ensures that our transformed data has the correct column names\n",
    "    feature_names = numerical_cols + list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_cols))\n",
    "\n",
    "    # Convert the transformed data back to DataFrames for easier use\n",
    "    X_train_df = pd.DataFrame(X_train_transformed, columns=feature_names)\n",
    "    X_test_df = pd.DataFrame(X_test_transformed, columns=feature_names)\n",
    "\n",
    "    return X_train_df, X_test_df, y_train, y_test,X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10391 entries, 0 to 10390\n",
      "Data columns (total 14 columns):\n",
      " #   Column                                  Non-Null Count  Dtype  \n",
      "---  ------                                  --------------  -----  \n",
      " 0   schoolid                                10391 non-null  int64  \n",
      " 1   Z                                       10391 non-null  int64  \n",
      " 2   Y                                       10391 non-null  float64\n",
      " 3   S3                                      10391 non-null  int64  \n",
      " 4   C1                                      10391 non-null  int64  \n",
      " 5   C2                                      10391 non-null  int64  \n",
      " 6   C3                                      10391 non-null  int64  \n",
      " 7   XC                                      10391 non-null  int64  \n",
      " 8   X1                                      10391 non-null  float64\n",
      " 9   X2                                      10391 non-null  float64\n",
      " 10  X3                                      10391 non-null  float64\n",
      " 11  X4                                      10391 non-null  float64\n",
      " 12  X5                                      10391 non-null  float64\n",
      " 13  Mindset_School_Achievement_Interaction  10391 non-null  float64\n",
      "dtypes: float64(7), int64(7)\n",
      "memory usage: 1.1 MB\n",
      "None\n",
      "Dataset Description:\n",
      "           schoolid             Z             Y            S3            C1  \\\n",
      "count  10391.000000  10391.000000  10391.000000  10391.000000  10391.000000   \n",
      "mean      39.888846      0.325666     -0.096742      5.268117      5.223078   \n",
      "std       24.008975      0.468646      0.643009      1.120765      3.982963   \n",
      "min        1.000000      0.000000     -2.097420      1.000000      1.000000   \n",
      "25%       19.000000      0.000000     -0.548980      5.000000      3.000000   \n",
      "50%       41.000000      0.000000     -0.118923      5.000000      4.000000   \n",
      "75%       62.000000      1.000000      0.335663      6.000000      5.000000   \n",
      "max       76.000000      1.000000      2.194709      7.000000     15.000000   \n",
      "\n",
      "                 C2            C3            XC            X1            X2  \\\n",
      "count  10391.000000  10391.000000  10391.000000  10391.000000  10391.000000   \n",
      "mean       1.489943      0.630931      2.447791     -0.040457      0.054841   \n",
      "std        0.499923      0.482576      1.378420      0.969743      0.935560   \n",
      "min        1.000000      0.000000      0.000000     -3.088790     -3.347819   \n",
      "25%        1.000000      0.000000      1.000000     -0.617888     -0.544506   \n",
      "50%        1.000000      1.000000      2.000000     -0.009954     -0.022514   \n",
      "75%        2.000000      1.000000      4.000000      0.420441      0.726836   \n",
      "max        2.000000      1.000000      4.000000      2.834589      2.171815   \n",
      "\n",
      "                 X3            X4            X5  \\\n",
      "count  10391.000000  10391.000000  10391.000000   \n",
      "mean      -0.089349     -0.045911     -0.026168   \n",
      "std        0.962804      0.967262      1.010387   \n",
      "min       -1.575463     -1.924778     -1.805073   \n",
      "25%       -0.963095     -0.813799     -0.857026   \n",
      "50%       -0.057036     -0.159602     -0.211553   \n",
      "75%        0.515392      0.596474      0.847844   \n",
      "max        2.358274      2.821660      1.892348   \n",
      "\n",
      "       Mindset_School_Achievement_Interaction  \n",
      "count                            10391.000000  \n",
      "mean                                -0.522748  \n",
      "std                                  1.057184  \n",
      "min                                 -8.911296  \n",
      "25%                                 -0.772797  \n",
      "50%                                 -0.302121  \n",
      "75%                                  0.015306  \n",
      "max                                  0.900523  \n",
      "Transformed Training Set:\n",
      "         X1        X2        X3        X4        X5  \\\n",
      "0 -0.510445  1.466730  0.051898 -0.967223  1.590097   \n",
      "1  0.384140  0.629177 -1.277115  0.276209 -0.402070   \n",
      "2 -0.321221  1.286003 -0.766009 -1.148723  0.818729   \n",
      "3  0.031335  0.751005 -0.148416  0.130397  0.365255   \n",
      "4 -0.510445  1.466730  0.051898 -0.967223  1.590097   \n",
      "\n",
      "   Mindset_School_Achievement_Interaction  C1_2  C1_3  C1_4  C1_5  ...  C1_12  \\\n",
      "0                               -0.226342   0.0   0.0   0.0   0.0  ...    0.0   \n",
      "1                                0.702910   0.0   0.0   1.0   0.0  ...    0.0   \n",
      "2                                0.078730   0.0   0.0   0.0   0.0  ...    0.0   \n",
      "3                                0.492070   0.0   0.0   1.0   0.0  ...    0.0   \n",
      "4                               -0.226342   0.0   0.0   1.0   0.0  ...    0.0   \n",
      "\n",
      "   C1_13  C1_14  C1_15  C2_2  C3_1  XC_1  XC_2  XC_3  XC_4  \n",
      "0    0.0    0.0    0.0   0.0   0.0   1.0   0.0   0.0   0.0  \n",
      "1    0.0    0.0    0.0   1.0   1.0   0.0   0.0   0.0   1.0  \n",
      "2    0.0    0.0    0.0   0.0   1.0   0.0   1.0   0.0   0.0  \n",
      "3    0.0    0.0    0.0   0.0   1.0   0.0   1.0   0.0   0.0  \n",
      "4    0.0    0.0    0.0   1.0   1.0   1.0   0.0   0.0   0.0  \n",
      "\n",
      "[5 rows x 26 columns]\n",
      "\n",
      "Transformed Test Set:\n",
      "         X1        X2        X3        X4        X5  \\\n",
      "0  1.057072 -0.781669  1.713448  0.283093 -1.473120   \n",
      "1 -0.014951 -0.643397  1.441220 -1.743257 -0.762711   \n",
      "2  1.650047 -1.152172  0.462079  1.121334 -0.445150   \n",
      "3 -1.227630  0.576547  0.092037  1.351892  0.965193   \n",
      "4  1.049241 -0.542535  0.178085 -0.632087 -0.300609   \n",
      "\n",
      "   Mindset_School_Achievement_Interaction  C1_2  C1_3  C1_4  C1_5  ...  C1_12  \\\n",
      "0                               -0.128421   1.0   0.0   0.0   0.0  ...    0.0   \n",
      "1                                0.527391   1.0   0.0   0.0   0.0  ...    0.0   \n",
      "2                               -1.007105   0.0   0.0   1.0   0.0  ...    0.0   \n",
      "3                               -0.198014   0.0   0.0   0.0   1.0  ...    0.0   \n",
      "4                                0.083536   0.0   0.0   0.0   0.0  ...    0.0   \n",
      "\n",
      "   C1_13  C1_14  C1_15  C2_2  C3_1  XC_1  XC_2  XC_3  XC_4  \n",
      "0    0.0    0.0    0.0   0.0   1.0   0.0   0.0   1.0   0.0  \n",
      "1    0.0    0.0    0.0   0.0   1.0   0.0   0.0   0.0   0.0  \n",
      "2    0.0    0.0    0.0   1.0   1.0   0.0   0.0   0.0   1.0  \n",
      "3    0.0    0.0    0.0   1.0   1.0   0.0   0.0   0.0   1.0  \n",
      "4    0.0    0.0    0.0   1.0   1.0   0.0   0.0   0.0   1.0  \n",
      "\n",
      "[5 rows x 26 columns]\n",
      "\n",
      "Training data shape: (8312, 26), Test data shape: (2079, 26)\n"
     ]
    }
   ],
   "source": [
    "X_train_df, X_test_df, y_train, y_test,X_train, X_test = data_preprocessing(dataset)\n",
    "\n",
    "# Step 7: Output the results to verify the transformations\n",
    "print(\"Transformed Training Set:\")\n",
    "print(X_train_df.head())\n",
    "print(\"\\nTransformed Test Set:\")\n",
    "print(X_test_df.head())\n",
    "\n",
    "# Output the shapes of the datasets\n",
    "print(f\"\\nTraining data shape: {X_train_df.shape}, Test data shape: {X_test_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S-Learner MSE: 0.45278502287687317\n",
      "Estimated treatment effects (S-Learner):\n",
      " [0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Function to handle missing values and train the S-Learner\n",
    "def s_learner(X_train, X_test, y_train, y_test, treatment_train, treatment_test,s_model):\n",
    "    # Append treatment column to training and test sets\n",
    "    X_train_with_treatment = X_train.copy()\n",
    "    X_test_with_treatment = X_test.copy()\n",
    "    \n",
    "    # Add treatment indicator to the feature set\n",
    "    X_train_with_treatment['treatment'] = treatment_train\n",
    "    X_test_with_treatment['treatment'] = treatment_test\n",
    "    \n",
    "    # Handle missing values by imputing them\n",
    "    imputer = SimpleImputer(strategy='mean')  # For numerical values\n",
    "    X_train_imputed = imputer.fit_transform(X_train_with_treatment)\n",
    "    X_test_imputed = imputer.transform(X_test_with_treatment)\n",
    "    \n",
    "    # Train the model on the combined feature set (covariates + treatment indicator)\n",
    "    s_model.fit(X_train_imputed, y_train)\n",
    "    \n",
    "    # Predict outcomes for both treated and untreated cases\n",
    "    X_test_with_treatment['treatment'] = 1  # Treated case\n",
    "    y_pred_treated = s_model.predict(X_test_imputed)\n",
    "    \n",
    "    X_test_with_treatment['treatment'] = 0  # Untreated case\n",
    "    y_pred_control = s_model.predict(X_test_imputed)\n",
    "    \n",
    "    # Calculate the treatment effect (difference between treated and control)\n",
    "    treatment_effect = y_pred_treated - y_pred_control\n",
    "    \n",
    "    # Evaluate the model performance using mean squared error (MSE)\n",
    "    mse = mean_squared_error(y_test, s_model.predict(X_test_imputed))\n",
    "    print(f\"S-Learner MSE: {mse}\")\n",
    "    \n",
    "    return treatment_effect, s_model\n",
    "\n",
    "# Extract treatment column (Z) for both training and testing datasets\n",
    "treatment_train = dataset.loc[X_train.index, 'Z']\n",
    "treatment_test = dataset.loc[X_test.index, 'Z']\n",
    "\n",
    " # Use a RandomForestRegressor as the base model for the S-learner\n",
    "s_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train and evaluate the S-learner with missing value handling\n",
    "s_treatment_effect, s_model = s_learner(X_train_df, X_test_df, y_train, y_test, treatment_train, treatment_test,s_model)\n",
    "\n",
    "# Print the estimated treatment effects\n",
    "print(\"Estimated treatment effects (S-Learner):\\n\", s_treatment_effect[:5])  # Showing first 5 treatment effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S-Learner MSE: 0.520193741079241\n",
      "Estimated treatment effects (S-Learner, Balanced):\n",
      " [0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Combine training data with treatment indicator and target\n",
    "X_train_with_treatment = X_train_df.copy()\n",
    "X_train_with_treatment['Z'] = treatment_train\n",
    "X_train_with_treatment['Y'] = y_train  # Add the target to the dataset\n",
    "\n",
    "# Split treated and untreated groups\n",
    "treated = X_train_with_treatment[X_train_with_treatment['Z'] == 1]\n",
    "untreated = X_train_with_treatment[X_train_with_treatment['Z'] == 0]\n",
    "\n",
    "# Oversample treated group to match the size of the untreated group\n",
    "treated_oversampled = resample(treated, \n",
    "                               replace=True,  # Sample with replacement\n",
    "                               n_samples=len(untreated),  # Match untreated sample size\n",
    "                               random_state=42)\n",
    "\n",
    "# Combine the oversampled treated group with the untreated group\n",
    "balanced_train = pd.concat([untreated, treated_oversampled])\n",
    "\n",
    "# Separate the features, treatment labels, and target again\n",
    "X_train_balanced = balanced_train.drop(columns=['Z', 'Y'])  # Features\n",
    "treatment_train_balanced = balanced_train['Z']  # Treatment indicator\n",
    "y_train_balanced = balanced_train['Y']  # Target (Outcome)\n",
    "\n",
    " # Use a RandomForestRegressor as the base model for the S-learner\n",
    "s_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Now train the S-learner on the balanced dataset\n",
    "s_treatment_effect_balanced, s_model_balanced = s_learner(X_train_balanced, X_test_df, y_train_balanced, y_test, treatment_train_balanced, treatment_test,s_model)\n",
    "\n",
    "# Print the estimated treatment effects\n",
    "print(\"Estimated treatment effects (S-Learner, Balanced):\\n\", s_treatment_effect_balanced[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S-Learner MSE: 0.37981158788422975\n",
      "Estimated treatment effects (S-Learner, with Propensity Scores):\n",
      " [0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dell\\OneDrive\\Desktop\\Thesis\\CATE_Estimation_using_Meta_Learners\\venv\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\dell\\OneDrive\\Desktop\\Thesis\\CATE_Estimation_using_Meta_Learners\\venv\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\dell\\OneDrive\\Desktop\\Thesis\\CATE_Estimation_using_Meta_Learners\\venv\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Step 1: Estimate propensity scores using Logistic Regression\n",
    "propensity_model = LogisticRegression(random_state=42)\n",
    "propensity_model.fit(X_train_df, treatment_train)\n",
    "\n",
    "# Get propensity scores for training and test sets\n",
    "propensity_scores_train = propensity_model.predict_proba(X_train_df)[:, 1]\n",
    "propensity_scores_test = propensity_model.predict_proba(X_test_df)[:, 1]\n",
    "\n",
    "# Step 2: Add propensity scores to the training and test datasets\n",
    "X_train_with_ps = X_train_df.copy()\n",
    "X_train_with_ps['propensity_score'] = propensity_scores_train\n",
    "\n",
    "X_test_with_ps = X_test_df.copy()\n",
    "X_test_with_ps['propensity_score'] = propensity_scores_test\n",
    "\n",
    "# Step 3: Handle missing values by imputing them\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train_with_ps)\n",
    "X_test_imputed = imputer.transform(X_test_with_ps)\n",
    "\n",
    "# Step 4: Train the S-Learner using Gradient Boosting\n",
    "s_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "s_model.fit(X_train_with_ps, y_train)\n",
    "\n",
    "# Predict outcomes for treated and untreated cases to calculate treatment effects\n",
    "X_test_with_ps['treatment'] = 1  # For treated cases\n",
    "y_pred_treated = s_model.predict(X_test_imputed)\n",
    "\n",
    "X_test_with_ps['treatment'] = 0  # For untreated cases\n",
    "y_pred_control = s_model.predict(X_test_imputed)\n",
    "\n",
    "# Calculate the treatment effect\n",
    "treatment_effect = y_pred_treated - y_pred_control\n",
    "\n",
    "# Step 5: Evaluate the model performance using mean squared error (MSE)\n",
    "mse = mean_squared_error(y_test, s_model.predict(X_test_imputed))\n",
    "print(f\"S-Learner MSE: {mse}\")\n",
    "\n",
    "# Print the estimated treatment effects\n",
    "print(\"Estimated treatment effects (S-Learner, with Propensity Scores):\\n\", treatment_effect[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S-Learner MSE: 0.3864978186060779\n",
      "Estimated treatment effects (S-Learner, Polynomial Features):\n",
      " [0.16798218 0.16823798 0.35765285 0.43156245 0.28123992]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Estimate propensity scores using Logistic Regression\n",
    "propensity_model = LogisticRegression(random_state=42)\n",
    "propensity_model.fit(X_train_df, treatment_train)\n",
    "\n",
    "# Get propensity scores for training and test sets\n",
    "propensity_scores_train = propensity_model.predict_proba(X_train_df)[:, 1]\n",
    "propensity_scores_test = propensity_model.predict_proba(X_test_df)[:, 1]\n",
    "\n",
    "# Step 2: Add propensity scores to the training and test datasets\n",
    "X_train_with_ps = X_train_df.copy()\n",
    "X_train_with_ps['propensity_score'] = propensity_scores_train\n",
    "\n",
    "X_test_with_ps = X_test_df.copy()\n",
    "X_test_with_ps['propensity_score'] = propensity_scores_test\n",
    "\n",
    "# Step 3: Handle missing values by imputing them\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train_with_ps)\n",
    "X_test_imputed = imputer.transform(X_test_with_ps)\n",
    "\n",
    "# Step 4: Create polynomial features\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train_imputed)\n",
    "X_test_poly = poly.transform(X_test_imputed)\n",
    "\n",
    "# Step 5: Add treatment column to imputed numpy arrays using np.column_stack\n",
    "X_train_poly_with_treatment = np.column_stack((X_train_poly, treatment_train))\n",
    "X_test_poly_with_treatment = np.column_stack((X_test_poly, treatment_test))\n",
    "\n",
    "# Step 6: Train the S-Learner using Gradient Boosting\n",
    "s_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "s_model.fit(X_train_poly_with_treatment, y_train)\n",
    "\n",
    "# Predict outcomes for both treated and untreated cases\n",
    "X_test_poly_with_treatment[:, -1] = 1  # For treated cases\n",
    "y_pred_treated = s_model.predict(X_test_poly_with_treatment)\n",
    "\n",
    "X_test_poly_with_treatment[:, -1] = 0  # For untreated cases\n",
    "y_pred_control = s_model.predict(X_test_poly_with_treatment)\n",
    "\n",
    "# Calculate the treatment effect\n",
    "treatment_effect = y_pred_treated - y_pred_control\n",
    "\n",
    "# Step 7: Evaluate the model performance using mean squared error (MSE)\n",
    "mse = mean_squared_error(y_test, s_model.predict(X_test_poly_with_treatment))\n",
    "print(f\"S-Learner MSE: {mse}\")\n",
    "\n",
    "# Print the estimated treatment effects\n",
    "print(\"Estimated treatment effects (S-Learner, Polynomial Features):\\n\", treatment_effect[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
