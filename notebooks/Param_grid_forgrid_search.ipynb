{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selected Parameter Grid for GridSearchCV:\n",
    "1. Random Forest Regressor\n",
    "Random Forests are non-parametric and handle categorical and continuous variables well. Given the nature of the dataset, we should focus on tuning the number of estimators, depth of trees, and other parameters.\n",
    "param_grids['RandomForest'] = {\n",
    "    'model': RandomForestRegressor(),\n",
    "    'params': {\n",
    "        'n_estimators': [100, 200, 500],  # Given dataset size, higher estimators might stabilize predictions.\n",
    "        'max_depth': [None, 10, 20],  # Max depth controls overfitting.\n",
    "        'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split a node.\n",
    "        'max_features': ['sqrt', 'log2', None]  # Limit the number of features considered for the best split.\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Gradient Boosting Regressor\n",
    "Gradient Boosting is sensitive to hyperparameters like learning rate and number of estimators. This model is great for handling non-linear relationships.\n",
    "param_grids['GradientBoosting'] = {\n",
    "    'model': GradientBoostingRegressor(),\n",
    "    'params': {\n",
    "        'n_estimators': [100, 200, 500],  # Number of boosting stages.\n",
    "        'learning_rate': [0.01, 0.05, 0.1],  # Smaller learning rates with more estimators.\n",
    "        'max_depth': [3, 5, 10],  # Control tree complexity.\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'subsample': [0.8, 1.0]  # Use subsample to prevent overfitting.\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Neural Network (MLP Regressor)\n",
    "Given that neural networks can be more sensitive to scaling and feature distribution, we should focus on layer sizes, activation functions, and solvers. You already observed convergence issues in some cases, so we can tweak learning rates and increase iterations.\n",
    "param_grids['NeuralNetwork'] = {\n",
    "    'model': MLPRegressor(max_iter=1000),  # Increase iterations to allow convergence.\n",
    "    'params': {\n",
    "        'hidden_layer_sizes': [(50,), (100,), (100, 100)],  # Different architectures for depth.\n",
    "        'activation': ['relu', 'tanh'],  # Test different activation functions.\n",
    "        'solver': ['adam', 'lbfgs'],  # Adam is more adaptive, lbfgs is better for small datasets.\n",
    "        'learning_rate_init': [0.001, 0.01]  # Control step size in learning.\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Ridge Regression\n",
    "Ridge regression is a linear model with regularization. Since weâ€™re dealing with many features, tuning the regularization strength is critical.\n",
    "param_grids['Ridge'] = {\n",
    "    'model': Ridge(),\n",
    "    'params': {\n",
    "        'alpha': [0.01, 0.1, 1.0, 10.0, 100.0]  # Regularization strength.\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Support Vector Regressor (SVR)\n",
    "SVR can handle non-linearity well and is less sensitive to the number of features but requires careful tuning of the kernel and regularization.\n",
    "param_grids['SVR'] = {\n",
    "    'model': SVR(),\n",
    "    'params': {\n",
    "        'kernel': ['linear', 'rbf'],  # Linear or Radial Basis Function kernel.\n",
    "        'C': [0.1, 1.0, 10.0],  # Regularization parameter.\n",
    "        'epsilon': [0.1, 0.2, 0.5],  # Epsilon-tube for margin of error.\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. AdaBoost Regressor\n",
    "AdaBoost works well for imbalanced datasets and can reduce overfitting by applying weights to difficult examples.\n",
    "param_grids['AdaBoost'] = {\n",
    "    'model': AdaBoostRegressor(),\n",
    "    'params': {\n",
    "        'n_estimators': [50, 100, 200],  # Number of boosting stages.\n",
    "        'learning_rate': [0.01, 0.1, 0.5],  # Smaller rates may improve performance.\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Extra Trees Regressor\n",
    "Extra Trees builds multiple trees in parallel and typically requires less fine-tuning than Random Forest, but it can still benefit from adjusting the tree depth and number of trees.\n",
    "param_grids['ExtraTrees'] = {\n",
    "    'model': ExtraTreesRegressor(),\n",
    "    'params': {\n",
    "        'n_estimators': [100, 200, 500],  # More trees for better averaging.\n",
    "        'max_depth': [None, 10, 20],  # Limiting depth to prevent overfitting.\n",
    "        'min_samples_split': [2, 5, 10],  # Minimum samples for splitting nodes.\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
